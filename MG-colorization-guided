{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"vamos a implementar un colorizador guidado por promt. La idea principar era obtener una clasificacion de lo que aparece en la imagen (CLIP o BLIP) y luego pasarlo como promt, pero de esta manera se pierde informacion, ya que desde los embeddings de lo que aparece a la imagen creamos un texto (discreto) para luego volver a codificarlo para pasarselo al modelo. \n\nVamos a pasar directamente al modelo via cross-attention o concatenacion el contenido de la imágen.\n\ncomplementaciones: podemos decodificar el contenido de la img en texto para ofrecer salida tupla (img_colorized, text_content):","metadata":{}},{"cell_type":"markdown","source":"### Preproceso datos\n\nusamos imagenette pq es más pequeño","metadata":{}},{"cell_type":"code","source":"# Ejecuta esto en la primera celda\n!pip install datasets transformers","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom datasets import load_dataset\nfrom transformers import CLIPVisionModel, CLIPImageProcessor\nfrom PIL import Image\nimport numpy as np\n\n# --- CONFIGURACIÓN ---\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMG_SIZE = 224  # Tamaño estándar para CLIP y eficiente para U-Net\nBATCH_SIZE = 32 # Ajusta a 16 si te da error de memoria (OOM)\nEMBED_DIM = 768 # Dimensión del vector de CLIP (base)\n\nprint(f\"Usando dispositivo: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T11:40:20.496796Z","iopub.execute_input":"2025-12-02T11:40:20.497225Z","iopub.status.idle":"2025-12-02T11:41:09.779764Z","shell.execute_reply.started":"2025-12-02T11:40:20.497189Z","shell.execute_reply":"2025-12-02T11:41:09.778359Z"}},"outputs":[{"name":"stderr","text":"2025-12-02 11:40:40.028617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764675640.311199      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764675640.395265      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Usando dispositivo: cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- DATASET ---\n# Usamos 'imagenette' (un subset de ImageNet de alta calidad y ligero)\n# 'frgfm/imagenette' versión '320px' para que sea rápido descargar\nprint(\"Descargando Dataset (aprox 300MB - 1.5GB)...\")\ndataset_hf = load_dataset(\"frgfm/imagenette\", \"320px\", split=\"train\")\n\nclass ColorizationDataset(torch.utils.data.Dataset):\n    def __init__(self, hf_dataset, transform=None):\n        self.dataset = hf_dataset\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # 1. Obtener imagen original\n        img = self.dataset[idx]['image'].convert('RGB')\n        \n        # 2. Transformaciones (Resize, Crop, Tensor)\n        if self.transform:\n            img_tensor = self.transform(img) # [3, 224, 224] RGB\n            \n        # 3. Crear versión escala de grises\n        # Usamos transformación estándar de RGB a Gris (1 canal)\n        gray_tensor = transforms.functional.rgb_to_grayscale(img_tensor, num_output_channels=1)\n        \n        # Para que entre a la U-Net y CLIP, a veces es más fácil replicar el canal gris 3 veces\n        # Entrada U-Net: (3, 224, 224) donde R=G=B\n        gray_3ch = gray_tensor.repeat(3, 1, 1)\n        \n        return gray_3ch, img_tensor # Input (Gris), Target (Color)\n\n# Transformaciones\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(), # [0, 1]\n    transforms.Normalize((0.5,), (0.5,)) # [-1, 1] Para Tanh\n])\n\n# Crear Dataloader\ntrain_dataset = ColorizationDataset(dataset_hf, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n\nprint(f\"Dataset listo. Imágenes totales: {len(train_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CLIPFeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Usamos CLIP ViT-Base\n        self.model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Congelar pesos para no gastar memoria entrenando esto\n        for param in self.model.parameters():\n            param.requires_grad = False\n            \n    def forward(self, img_tensor):\n        # CLIP espera una normalización específica, pero para este prototipo rápido\n        # usaremos el tensor tal cual viene del loader (aproximación válida para test)\n        # img_tensor shape: [B, 3, 224, 224]\n        \n        # Obtenemos features\n        outputs = self.model(pixel_values=img_tensor)\n        # pooler_output es el vector resumen [B, 768]\n        return outputs.pooler_output\n\n# Instanciamos fuera del bucle\nclip_extractor = CLIPFeatureExtractor().to(DEVICE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"crear modelo","metadata":{}},{"cell_type":"code","source":"class ConditionalUNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # --- ENCODER ---\n        # Reduce tamaño, aumenta canales\n        self.enc1 = self.conv_block(3, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.pool = nn.MaxPool2d(2)\n        \n        # --- BOTTLENECK + FUSIÓN ---\n        # Aquí la imagen es 28x28 (224 / 2^3)\n        self.bottleneck = self.conv_block(256, 512)\n        \n        # Proyección del Embedding de CLIP para mezclarlo\n        # Queremos concatenar el embedding a la imagen latente\n        self.embed_proj = nn.Linear(EMBED_DIM, 512) \n        \n        # --- DECODER ---\n        # Aumenta tamaño, reduce canales\n        # Input channels = 512 (de abajo) + 512 (embedding) = 1024\n        self.up3 = nn.ConvTranspose2d(1024, 256, kernel_size=2, stride=2)\n        self.dec3 = self.conv_block(512, 256) # 256 del up + 256 del skip connection\n        \n        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec2 = self.conv_block(256, 128)\n        \n        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec1 = self.conv_block(128, 64)\n        \n        # Salida final RGB\n        self.final = nn.Conv2d(64, 3, kernel_size=1)\n        \n    def conv_block(self, in_c, out_c):\n        return nn.Sequential(\n            nn.Conv2d(in_c, out_c, 3, padding=1),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_c, out_c, 3, padding=1),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x, embed):\n        # x: [Batch, 3, 224, 224] (Grayscale replicado)\n        # embed: [Batch, 768]\n        \n        # Encoder\n        e1 = self.enc1(x)\n        p1 = self.pool(e1) # 112\n        \n        e2 = self.enc2(p1)\n        p2 = self.pool(e2) # 56\n        \n        e3 = self.enc3(p2)\n        p3 = self.pool(e3) # 28\n        \n        # Bottleneck\n        b = self.bottleneck(p3) # [B, 512, 28, 28]\n        \n        # --- FUSIÓN DE EMBEDDING ---\n        # 1. Proyectar embedding: [B, 768] -> [B, 512]\n        emb_vec = self.embed_proj(embed) \n        # 2. Replicar espacialmente para que coincida con la imagen (28x28)\n        emb_map = emb_vec.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 28, 28)\n        # 3. Concatenar: [B, 512, 28, 28] + [B, 512, 28, 28] -> [B, 1024, 28, 28]\n        fused = torch.cat([b, emb_map], dim=1)\n        \n        # Decoder\n        d3 = self.up3(fused)        # -> [B, 256, 56, 56]\n        d3 = torch.cat([d3, e3], dim=1) # Skip connection\n        d3 = self.dec3(d3)\n        \n        d2 = self.up2(d3)           # -> [B, 128, 112, 112]\n        d2 = torch.cat([d2, e2], dim=1)\n        d2 = self.dec2(d2)\n        \n        d1 = self.up1(d2)           # -> [B, 64, 224, 224]\n        d1 = torch.cat([d1, e1], dim=1)\n        d1 = self.dec1(d1)\n        \n        return torch.tanh(self.final(d1)) # Salida [-1, 1]\n\nmodel = ConditionalUNet().to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.MSELoss() # L2 Loss para colores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Entrenamiento","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nEPOCHS = 5 # Pocas épocas para probar rápido (sube a 20-50 para resultados reales)\nscaler = torch.cuda.amp.GradScaler() # Para Mixed Precision\n\nprint(\"Iniciando entrenamiento...\")\n\nfor epoch in range(EPOCHS):\n    model.train()\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    for gray_imgs, color_real in loop:\n        gray_imgs = gray_imgs.to(DEVICE)\n        color_real = color_real.to(DEVICE)\n        \n        # 1. Extraer Embedding de la imagen (usamos la gris para simular inferencia real)\n        # Truco: CLIP funciona mejor con la original a color, pero en inferencia \n        # real solo tendrás la gris. Si quieres robustez, usa la gris aquí también.\n        with torch.no_grad():\n            # Nota: CLIP espera RGB. gray_imgs ya tiene 3 canales repetidos.\n            embeddings = clip_extractor(gray_imgs) \n        \n        # 2. Forward Pass (Mixed Precision)\n        with torch.cuda.amp.autocast():\n            color_pred = model(gray_imgs, embeddings)\n            loss = criterion(color_pred, color_real)\n            \n        # 3. Backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        loop.set_postfix(loss=loss.item())\n\n# Guardar modelo\ntorch.save(model.state_dict(), \"colorizer_clip_model.pth\")\nprint(\"¡Entrenamiento completado!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Resultados","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nmodel.eval()\n# Coger un batch de ejemplo\ngray, real = next(iter(train_loader))\ngray = gray.to(DEVICE)\n\nwith torch.no_grad():\n    emb = clip_extractor(gray)\n    fake = model(gray, emb)\n\n# Función para desnormalizar y mostrar\ndef show_tensor(tensor_img):\n    img = tensor_img.cpu().clone()\n    img = img * 0.5 + 0.5 # Deshacer normalización [-1, 1]\n    img = img.permute(1, 2, 0).clamp(0, 1) # [C,H,W] -> [H,W,C]\n    return img\n\nfig, axs = plt.subplots(3, 3, figsize=(12, 12))\nfor i in range(3):\n    axs[i, 0].imshow(show_tensor(gray[i]), cmap='gray')\n    axs[i, 0].set_title(\"Input (Gris)\")\n    axs[i, 1].imshow(show_tensor(fake[i]))\n    axs[i, 1].set_title(\"Predicción (Con CLIP)\")\n    axs[i, 2].imshow(show_tensor(real[i]))\n    axs[i, 2].set_title(\"Real (Ground Truth)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}