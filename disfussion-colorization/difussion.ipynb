{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48e6d2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d6971",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset: devuelve pares RGB / Gray\n",
    "# -----------------------------\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, img_dir, size=512):\n",
    "        self.img_dir = img_dir\n",
    "        self.files = [f for f in os.listdir(img_dir) if f.endswith((\".jpg\",\".png\"))]\n",
    "        self.size = size\n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # [-1,1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.img_dir, self.files[idx])\n",
    "        img_rgb = Image.open(path).convert(\"RGB\")\n",
    "        img_gray = img_rgb.convert(\"L\").convert(\"RGB\")  # 3 canales para VAE\n",
    "\n",
    "        rgb_tensor = self.to_tensor(img_rgb)   # [3,H,W]\n",
    "        gray_tensor = self.to_tensor(img_gray) # [3,H,W]\n",
    "\n",
    "        return {\"rgb\": rgb_tensor, \"gray\": gray_tensor}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a988b",
   "metadata": {},
   "source": [
    "### Cargar modelos e hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d71f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Hiperparámetros\n",
    "# -----------------------------\n",
    "NUM_EPOCHS = 100\n",
    "EPOCHS_VISUALIZATION = 5\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-5\n",
    "\n",
    "# -----------------------------\n",
    "# Modelos: VAE congelado y UNet entrenable\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"vae\"\n",
    ").to(device)\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"unet\"\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Dataloader\n",
    "# -----------------------------\n",
    "dataset = ColorizationDataset(\"/ruta/a/imagenes\", size=512)\n",
    "# Dataset de validación\n",
    "val_dataset = ColorizationDataset(\"/ruta/a/imagenes_validacion\", size=512)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizador\n",
    "# -----------------------------\n",
    "opt = AdamW(unet.parameters(), lr=LR, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb41cc",
   "metadata": {},
   "source": [
    "### Bucle entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_validation(vae, unet, val_dataset, device=DEVICE, steps=50, color_scale=1.0, num_samples=5):\n",
    "    # Selecciona aleatoriamente num_samples imágenes de validación\n",
    "    indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 3*num_samples))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = val_dataset[idx]\n",
    "        # rgb = sample[\"rgb\"].unsqueeze(0).to(device)\n",
    "        gray = sample[\"gray\"].unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Codificar gris\n",
    "            z_gray = vae.encode(gray).latent_dist.sample() * 0.18215\n",
    "            z_t = z_gray.clone()\n",
    "\n",
    "            # Refinamiento paso a paso\n",
    "            ts = torch.linspace(1.0, 0.0, steps, device=device)\n",
    "            for t in ts:\n",
    "                t_int = torch.tensor([int(t.item()*999)], device=device)\n",
    "                text_emb = torch.zeros((1,77,768), device=device)\n",
    "                delta_t = unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_emb).sample\n",
    "                z_t = z_t + (1.0/steps) * delta_t\n",
    "\n",
    "            z_col = z_gray + color_scale * (z_t - z_gray)\n",
    "            out_rgb = vae.decode(z_col / 0.18215).sample.squeeze(0)\n",
    "\n",
    "        # Conversión a imágenes para mostrar\n",
    "        def tensor_to_img(t):\n",
    "            arr = t.detach().cpu().permute(1,2,0).numpy()\n",
    "            arr = (arr*0.5+0.5).clip(0,1)\n",
    "            return arr\n",
    "\n",
    "        rgb_img = tensor_to_img(sample[\"rgb\"])\n",
    "        gray_img = tensor_to_img(sample[\"gray\"])\n",
    "        colorized_img = tensor_to_img(out_rgb)\n",
    "\n",
    "        axes[i,0].imshow(gray_img)\n",
    "        axes[i,0].set_title(\"Grayscale input\")\n",
    "        axes[i,0].axis(\"off\")\n",
    "\n",
    "        axes[i,1].imshow(colorized_img)\n",
    "        axes[i,1].set_title(\"Colorized output\")\n",
    "        axes[i,1].axis(\"off\")\n",
    "\n",
    "        axes[i,2].imshow(rgb_img)\n",
    "        axes[i,2].set_title(\"Original RGB\")\n",
    "        axes[i,2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in loader:\n",
    "        rgb_batch = batch[\"rgb\"].to(DEVICE)   # [B,3,H,W]\n",
    "        gray_batch = batch[\"gray\"].to(DEVICE) # [B,3,H,W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode latentes\n",
    "            z_rgb = vae.encode(rgb_batch).latent_dist.sample() * 0.18215\n",
    "            z_gray = vae.encode(gray_batch).latent_dist.sample() * 0.18215\n",
    "\n",
    "            # Mezcla temporal\n",
    "            t = torch.rand(z_rgb.size(0), device=DEVICE)\n",
    "            z_t = (1 - t.view(-1,1,1,1)) * z_gray + t.view(-1,1,1,1) * z_rgb\n",
    "            target = z_rgb\n",
    "\n",
    "        # Diffusers espera timesteps enteros (0-999)\n",
    "        t_int = torch.randint(0, 1000, (rgb_batch.size(0),), device=DEVICE)\n",
    "\n",
    "        # Dummy text conditioning (sin texto)\n",
    "        text_emb = torch.zeros((rgb_batch.size(0), 77, 768), device=DEVICE)\n",
    "\n",
    "        # Predicción del residuo\n",
    "        delta_hat = unet(\n",
    "            sample=z_t,\n",
    "            timestep=t_int,\n",
    "            encoder_hidden_states=text_emb\n",
    "        ).sample\n",
    "\n",
    "        # Reconstrucción y loss\n",
    "        recon = z_t + delta_hat\n",
    "        loss = F.mse_loss(recon, target)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if (epoch + 1) % EPOCHS_VISUALIZATION == 0:\n",
    "        visualize_validation(vae, unet, val_dataset, device=DEVICE, steps=50, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9049ee",
   "metadata": {},
   "source": [
    "## Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c63461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer:\n",
    "    def __init__(self, vae: AutoencoderKL, unet: UNet2DConditionModel, device='cuda'):\n",
    "        self.vae = vae.eval().to(device)\n",
    "        self.unet = unet.eval().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _preprocess_gray(self, img_gray_pil, size=512):\n",
    "        img = img_gray_pil.resize((size, size), Image.BICUBIC)\n",
    "        arr = np.array(img.convert(\"L\").convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "        arr = (arr - 0.5) / 0.5\n",
    "        return torch.from_numpy(arr).permute(2,0,1).unsqueeze(0).to(self.device)  # [1,3,H,W]\n",
    "\n",
    "    def _replace_luma(self, out_rgb_norm, in_gray_norm):\n",
    "        out = out_rgb_norm.permute(1,2,0).cpu().numpy()*0.5+0.5\n",
    "        gray = in_gray_norm[0].cpu().numpy()*0.5+0.5\n",
    "        out_bgr = cv2.cvtColor((out*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "        lab = cv2.cvtColor(out_bgr, cv2.COLOR_BGR2Lab)\n",
    "        lab[:,:,0] = (gray*255).astype(np.uint8)\n",
    "        bgr = cv2.cvtColor(lab, cv2.COLOR_Lab2BGR)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        return Image.fromarray(rgb)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def colorize(self, gray_img_pil: Image.Image, steps=50, size=512, color_scale=1.0):\n",
    "        # 1) Preprocesar y codificar\n",
    "        gray = self._preprocess_gray(gray_img_pil, size)  # [1,3,H,W]\n",
    "        z_gray = self.vae.encode(gray).latent_dist.sample() * 0.18215\n",
    "        z_t = z_gray.clone()\n",
    "\n",
    "        # 2) Iteración de refinamiento\n",
    "        ts = torch.linspace(1.0, 0.0, steps, device=self.device)\n",
    "        for t in ts:\n",
    "            t_int = torch.tensor([int(t.item()*999)], device=self.device)\n",
    "            text_emb = torch.zeros((1,77,768), device=self.device)  # dummy text conditioning\n",
    "            delta_t = self.unet(sample=z_t, timestep=t_int, encoder_hidden_states=text_emb).sample\n",
    "            z_t = z_t + (1.0/steps) * delta_t\n",
    "\n",
    "        # 3) Escalado y decodificación\n",
    "        z_col = z_gray + color_scale * (z_t - z_gray)\n",
    "        out_rgb = self.vae.decode(z_col / 0.18215).sample.squeeze(0)\n",
    "\n",
    "        # 4) Reemplazo de luminancia\n",
    "        final_img = self._replace_luma(out_rgb, gray.squeeze(0))\n",
    "        return final_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelos de diffusers\n",
    "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
    "\n",
    "colorizer = Colorizer(vae, unet, device=DEVICE)\n",
    "\n",
    "gray_img = Image.open(\"foto_gris.png\").convert(\"L\")\n",
    "colorized = colorizer.colorize(gray_img, steps=50, size=512, color_scale=1.0)\n",
    "colorized.save(\"foto_color.png\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
